{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ce6f328",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "import model.sdes as sdes\n",
    "import model.generate as generate\n",
    "import model.table_dnn as table_dnn\n",
    "import model.util as model_util\n",
    "import torch\n",
    "import numpy as np\n",
    "import scipy.stats\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df22d86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define device\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = \"cuda\"\n",
    "else:\n",
    "    DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe528ba",
   "metadata": {},
   "source": [
    "### Define the branches and create the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e395da27",
   "metadata": {},
   "outputs": [],
   "source": [
    "letters = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\")\n",
    "class_to_letter = dict(enumerate(letters))\n",
    "letter_to_class = {v : k for k, v in class_to_letter.items()}\n",
    "\n",
    "classes = [letter_to_class[l] for l in letters]\n",
    "branch_defs = [(('A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z'), 0.5235235235235235, 1), (('A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'X', 'Y', 'Z'), 0.5165165165165165, 0.5235235235235235), (('B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'X', 'Y', 'Z'), 0.5115115115115115, 0.5165165165165165), (('B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'X', 'Y', 'Z'), 0.4944944944944945, 0.5115115115115115), (('I', 'J'), 0.4794794794794795, 0.4944944944944945), (('B', 'C', 'D', 'E', 'F', 'G', 'H', 'K', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'X', 'Y', 'Z'), 0.4724724724724725, 0.4944944944944945), (('B', 'C', 'D', 'E', 'G', 'H', 'K', 'M', 'N', 'O', 'Q', 'R', 'S', 'U', 'X', 'Z'), 0.45645645645645644, 0.4724724724724725), (('F', 'P', 'T', 'V', 'Y'), 0.4364364364364364, 0.4724724724724725), (('B', 'C', 'D', 'E', 'G', 'H', 'K', 'N', 'O', 'Q', 'R', 'S', 'U', 'X', 'Z'), 0.4174174174174174, 0.45645645645645644), (('B', 'C', 'D', 'E', 'G', 'H', 'K', 'N', 'O', 'Q', 'R', 'S', 'X', 'Z'), 0.4134134134134134, 0.4174174174174174), (('B', 'D', 'G', 'H', 'K', 'N', 'O', 'Q', 'R', 'S', 'X', 'Z'), 0.4094094094094094, 0.4134134134134134), (('F', 'T', 'V', 'Y'), 0.4024024024024024, 0.4364364364364364), (('B', 'D', 'G', 'H', 'K', 'O', 'Q', 'R', 'S', 'X', 'Z'), 0.3863863863863864, 0.4094094094094094), (('B', 'G', 'H', 'K', 'O', 'Q', 'R', 'S', 'X', 'Z'), 0.3813813813813814, 0.3863863863863864), (('B', 'G', 'H', 'O', 'Q', 'R', 'S', 'X', 'Z'), 0.3733733733733734, 0.3813813813813814), (('F', 'T', 'Y'), 0.36036036036036034, 0.4024024024024024), (('C', 'E'), 0.3563563563563564, 0.4134134134134134), (('T', 'Y'), 0.3533533533533533, 0.36036036036036034), (('B', 'R', 'S', 'X', 'Z'), 0.35135135135135137, 0.3733733733733734), (('G', 'H', 'O', 'Q'), 0.34134134134134136, 0.3733733733733734), (('B', 'S', 'X', 'Z'), 0.32232232232232233, 0.35135135135135137), (('B', 'S', 'X'), 0.27627627627627627, 0.32232232232232233), (('G', 'H', 'O'), 0.26426426426426425, 0.34134134134134136), (('G', 'O'), 0.25725725725725723, 0.26426426426426425), (('S', 'X'), 0.15615615615615616, 0.27627627627627627), (('W',), 0, 0.5235235235235235), (('A',), 0, 0.5165165165165165), (('L',), 0, 0.5115115115115115), (('J',), 0, 0.4794794794794795), (('I',), 0, 0.4794794794794795), (('M',), 0, 0.45645645645645644), (('P',), 0, 0.4364364364364364), (('U',), 0, 0.4174174174174174), (('N',), 0, 0.4094094094094094), (('V',), 0, 0.4024024024024024), (('D',), 0, 0.3863863863863864), (('K',), 0, 0.3813813813813814), (('F',), 0, 0.36036036036036034), (('E',), 0, 0.3563563563563564), (('C',), 0, 0.3563563563563564), (('Y',), 0, 0.3533533533533533), (('T',), 0, 0.3533533533533533), (('R',), 0, 0.35135135135135137), (('Q',), 0, 0.34134134134134136), (('Z',), 0, 0.32232232232232233), (('B',), 0, 0.27627627627627627), (('H',), 0, 0.26426426426426425), (('G',), 0, 0.25725725725725723), (('O',), 0, 0.25725725725725723), (('S',), 0, 0.15615615615615616), (('X',), 0, 0.15615615615615616)]\n",
    "\n",
    "# classes = [letter_to_class[l] for l in (\"X\", \"O\", \"Q\")]\n",
    "# branch_defs = [\n",
    "#     ((\"X\", \"O\", \"Q\"), 0.5, 1),\n",
    "#     ((\"X\",), 0, 0.5),\n",
    "#     ((\"O\", \"Q\"), 0.4, 0.5),\n",
    "#     ((\"O\",), 0, 0.4),\n",
    "#     ((\"Q\",), 0, 0.4)\n",
    "# ]\n",
    "\n",
    "# classes = [letter_to_class[l] for l in (\"A\", \"V\", \"Y\")]\n",
    "# branch_defs = [\n",
    "#     ((\"A\", \"V\", \"Y\"), 0.65, 1),\n",
    "#     ((\"A\",), 0, 0.65),\n",
    "#     ((\"V\", \"Y\"), 0.35, 0.65),\n",
    "#     ((\"V\",), 0, 0.35),\n",
    "#     ((\"Y\",), 0, 0.35)\n",
    "# ]\n",
    "\n",
    "# classes = [letter_to_class[\"O\"]]\n",
    "# branch_defs = [((\"O\",), 0, 1)]\n",
    "\n",
    "# classes = [letter_to_class[l] for l in letters]\n",
    "# branch_defs = [(tuple(letters), 0, 1)]\n",
    "\n",
    "branch_defs = [\n",
    "    (tuple(map(lambda l: letter_to_class[l], trip[0])), trip[1], trip[2])\n",
    "    for trip in branch_defs\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "951177e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LetterDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        data_path = \"/gstore/data/resbioai/tsenga5/branched_diffusion/data/letter_recognition/letter-recognition.data\"\n",
    "        \n",
    "        data = []\n",
    "        targets = []\n",
    "        with open(data_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                tokens = line.strip().split(\",\")\n",
    "                targets.append(tokens[0])\n",
    "                data.append(np.array(list(map(int, tokens[1:]))))\n",
    "        self.data = np.stack(data)\n",
    "        self.targets = np.array([letter_to_class[l] for l in targets])\n",
    "        \n",
    "        # Center/normalize the data\n",
    "        self.data = (self.data - np.mean(self.data, axis=0, keepdims=True)) / \\\n",
    "            np.std(self.data, axis=0, keepdims=True)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        return torch.tensor(self.data[index]).float(), self.targets[index]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.targets)\n",
    "    \n",
    "dataset = LetterDataset()\n",
    "\n",
    "# Limit classes\n",
    "inds = np.isin(dataset.targets, classes)\n",
    "dataset.data = dataset.data[inds]\n",
    "dataset.targets = dataset.targets[inds]\n",
    "\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "input_shape = next(iter(data_loader))[0].shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6d3f84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: this is currently rather inefficient; a decision-tree-style structure\n",
    "# would be better\n",
    "\n",
    "def class_time_to_branch(c, t):\n",
    "    \"\"\"\n",
    "    Given a class and a time (both scalars), return the\n",
    "    corresponding branch index.\n",
    "    \"\"\"\n",
    "    for i, branch_def in enumerate(branch_defs):\n",
    "        if c in branch_def[0] and t >= branch_def[1] and t <= branch_def[2]:\n",
    "            return i\n",
    "    raise ValueError(\"Undefined class and time\")\n",
    "        \n",
    "def class_time_to_branch_tensor(c, t):\n",
    "    \"\"\"\n",
    "    Given tensors of classes and a times, return the\n",
    "    corresponding branch indices as a tensor.\n",
    "    \"\"\"\n",
    "    return torch.tensor([\n",
    "        class_time_to_branch(c_i, t_i) for c_i, t_i in zip(c, t)\n",
    "    ], device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1a36e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the SDE and model\n",
    "sde = sdes.VariancePreservingSDE(0.1, 1, input_shape)\n",
    "\n",
    "t_limit = 1\n",
    "model = table_dnn.MultitaskTabularNet(\n",
    "    len(branch_defs), input_shape[0], t_limit=t_limit\n",
    ").to(DEVICE)\n",
    "\n",
    "os.environ[\"MODEL_DIR\"] = \"/gstore/data/resbioai/tsenga5/branched_diffusion/models/trained_models/letters_continuous_allletters\"\n",
    "\n",
    "import model.train_continuous_model as train_continuous_model  # Import this AFTER setting environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c07baf0",
   "metadata": {},
   "source": [
    "### Show the forward-diffusion process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efc4143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the transformation of the distribution of data to the prior distribution\n",
    "x0, _ = next(iter(data_loader))\n",
    "x0 = x0.cpu().numpy()\n",
    "x0 = torch.tensor(x0).to(DEVICE)\n",
    "\n",
    "time_steps = 30\n",
    "\n",
    "all_t = np.linspace(0, t_limit, time_steps)\n",
    "all_xt = np.empty((len(all_t),) + x0.shape)\n",
    "for t_i, t in enumerate(all_t):\n",
    "    xt, _ = sde.forward(x0, torch.ones(len(x0)).to(DEVICE) * t)\n",
    "    all_xt[t_i] = xt.cpu().numpy()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20, 8))\n",
    "cmap = plt.get_cmap(\"magma\")\n",
    "for t_i in range(len(all_t)):\n",
    "    ax.hist(np.ravel(all_xt[t_i]), bins=60, histtype=\"step\", color=cmap(t_i / len(all_t)), alpha=0.5, density=True)\n",
    "prior = sde.sample_prior(len(x0), torch.ones(len(x0)).to(DEVICE) * t).cpu().numpy()\n",
    "ax.hist(np.ravel(prior), bins=60, histtype=\"step\", color=\"blue\", linewidth=2, density=True, label=\"Sampled prior\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"p(x)\")\n",
    "ax.set_title(\"Evolution of p(x) over forward SDE\")\n",
    "ax.set_ylim((0, 3))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f19ff6b",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f13f9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_continuous_model.train_ex.run(\n",
    "    \"train_branched_model\",\n",
    "    config_updates={\n",
    "        \"model\": model,\n",
    "        \"sde\": sde,\n",
    "        \"data_loader\": data_loader,\n",
    "        \"class_time_to_branch_index\": class_time_to_branch_tensor,\n",
    "        \"num_epochs\": 100,\n",
    "        \"learning_rate\": 1e-3,\n",
    "        \"t_limit\": t_limit,\n",
    "        \"loss_weighting_type\": \"empirical_norm\"\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f84775",
   "metadata": {},
   "source": [
    "### Show generated distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6726f308",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = {}\n",
    "for class_to_sample in classes:\n",
    "    sample = generate.generate_continuous_branched_samples(\n",
    "        model, sde, class_to_sample, class_time_to_branch_tensor,\n",
    "        sampler=\"pc\", t_limit=t_limit\n",
    "    ).cpu().numpy()\n",
    "    samples[class_to_sample] = sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa6bbb3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# For simplicity, sample a lot of inputs for the real data\n",
    "data, targets = [], []\n",
    "for _ in range(len(classes)):\n",
    "    x, y = next(iter(data_loader))\n",
    "    data.append(x.numpy())\n",
    "    targets.append(y.numpy())\n",
    "data = np.concatenate([data])\n",
    "targets = np.concatenate([targets])\n",
    "\n",
    "for class_to_sample in classes:\n",
    "    print(\"Class %s\" % class_to_letter[class_to_sample])\n",
    "    real = data[targets == class_to_sample]\n",
    "    sample = samples[class_to_sample]\n",
    "    \n",
    "    num_features = input_shape[0]\n",
    "    num_cols = 4\n",
    "    num_rows = int(np.ceil(num_features / num_cols))\n",
    "    num_bins = 20\n",
    "\n",
    "    fig, ax = plt.subplots(ncols=num_cols, nrows=num_rows, figsize=(num_cols * 5, num_rows * 5))\n",
    "    for i in range(num_features):\n",
    "        r, c = i // num_cols, i % num_cols\n",
    "        real_vals = real[:, i]\n",
    "        sample_vals = sample[:, i]\n",
    "        all_vals = np.concatenate([real_vals, sample_vals])\n",
    "        bins = np.linspace(np.min(all_vals), np.max(all_vals), num_bins)\n",
    "        ax[r][c].hist(real_vals, bins=bins, color=\"royalblue\", label=\"Real\", density=True, alpha=0.5)\n",
    "        ax[r][c].hist(sample_vals, bins=bins, color=\"darkorange\", label=\"Samples\", density=True, alpha=0.5)\n",
    "        ax[r][c].set_title(\"Feature %d\" % (i + 1))\n",
    "    ax[0][0].legend()\n",
    "    plt.show()\n",
    "\n",
    "    real_corrs = np.empty((num_features, num_features))\n",
    "    sample_corrs = np.empty_like(real_corrs)\n",
    "    for i in range(num_features):\n",
    "        real_corrs[i, i] = 1\n",
    "        sample_corrs[i, i] = 1\n",
    "        for j in range(i):\n",
    "            real_corrs[i, j] = scipy.stats.pearsonr(real[:, i], real[:, j])[0]\n",
    "            real_corrs[j, i] = real_corrs[i, j]\n",
    "            sample_corrs[i, j] = scipy.stats.pearsonr(sample[:, i], sample[:, j])[0]\n",
    "            sample_corrs[j, i] = sample_corrs[i, j]\n",
    "\n",
    "    fig, ax = plt.subplots(ncols=3, figsize=(20, 10))\n",
    "    ax[0].imshow(real_corrs, cmap=\"Blues\", vmin=0, vmax=1)\n",
    "    ax[0].set_title(\"Real correlations\")\n",
    "    ax[1].imshow(sample_corrs, cmap=\"Blues\", vmin=0, vmax=1)\n",
    "    ax[1].set_title(\"Sample correlations\")\n",
    "    ax[2].imshow(np.abs(real_corrs - sample_corrs), cmap=\"Blues\", vmin=0, vmax=1)\n",
    "    ax[2].set_title(\"Absolute difference in correlations\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f80b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of features by class\n",
    "num_features = input_shape[0]\n",
    "num_cols = 4\n",
    "num_rows = int(np.ceil(num_features / num_cols))\n",
    "num_bins = 20\n",
    "fig, ax = plt.subplots(ncols=num_cols, nrows=num_rows, figsize=(num_cols * 5, num_rows * 5))\n",
    "\n",
    "for i in range(num_features):\n",
    "    r, c = i // num_cols, i % num_cols\n",
    "    data_to_plot = {}\n",
    "    for class_to_sample in classes:\n",
    "        data_to_plot[class_to_sample] = data[targets == class_to_sample][:, i]\n",
    "    all_vals = np.concatenate(list(data_to_plot.values()))\n",
    "    bins = np.linspace(np.min(all_vals), np.max(all_vals), num_bins)\n",
    "    for class_to_sample in classes:\n",
    "        ax[r][c].hist(\n",
    "            data_to_plot[class_to_sample], bins=bins, label=class_to_letter[class_to_sample],\n",
    "            density=True, alpha=0.5\n",
    "        )\n",
    "    ax[r][c].set_title(\"Feature %d\" % (i + 1))\n",
    "ax[0][0].legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4dc1e9",
   "metadata": {},
   "source": [
    "**Diffusing from one class to another**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87136dba",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "branch_time = 0.35\n",
    "c1, c2 = \"V\", \"Y\"\n",
    "\n",
    "num_features = input_shape[0]\n",
    "\n",
    "orig_c1 = torch.stack([\n",
    "    dataset[i][0].to(DEVICE) for i in \n",
    "    np.random.choice(np.where(dataset.targets == letter_to_class[c1])[0], size=512, replace=False)\n",
    "])\n",
    "orig_c2 = torch.stack([\n",
    "    dataset[i][0].to(DEVICE) for i in \n",
    "    np.random.choice(np.where(dataset.targets == letter_to_class[c2])[0], size=512, replace=False)\n",
    "])\n",
    "\n",
    "time_steps = 10\n",
    "all_t = np.linspace(0, branch_time, time_steps)\n",
    "forward_c1_to_c2 = np.empty((len(all_t),) + orig_c1.shape)\n",
    "backward_c1_to_c2 = np.empty((len(all_t) - 1,) + orig_c1.shape)\n",
    "forward_c2_to_c1 = np.empty((len(all_t),) + orig_c2.shape)\n",
    "backward_c2_to_c1 = np.empty((len(all_t) - 1,) + orig_c2.shape)\n",
    "for t_i, t in enumerate(all_t):\n",
    "    forward_c1_to_c2[t_i] = sde.forward(\n",
    "        orig_c1, torch.ones(len(orig_c1)).to(DEVICE) * t\n",
    "    )[0].cpu().numpy()\n",
    "    forward_c2_to_c1[t_i] = sde.forward(\n",
    "        orig_c2, torch.ones(len(orig_c2)).to(DEVICE) * t\n",
    "    )[0].cpu().numpy()\n",
    "\n",
    "all_t_flip = np.flip(all_t)\n",
    "last_c1_to_c2 = torch.tensor(forward_c1_to_c2[-1]).to(DEVICE).float()\n",
    "last_c2_to_c1 = torch.tensor(forward_c2_to_c1[-1]).to(DEVICE).float()\n",
    "for t_i in range(len(all_t_flip) - 1):\n",
    "    last_c1_to_c2 = generate.generate_continuous_branched_samples(\n",
    "        model, sde, letter_to_class[c2], class_time_to_branch_tensor, sampler=\"pc\",\n",
    "        t_limit=all_t_flip[t_i], t_start=all_t_flip[t_i + 1],\n",
    "        num_samples=orig_c1.shape[0], initial_samples=last_c1_to_c2,\n",
    "        num_steps=50\n",
    "    )\n",
    "    backward_c1_to_c2[t_i] = last_c1_to_c2.cpu().numpy()\n",
    "    \n",
    "    last_c2_to_c1 = generate.generate_continuous_branched_samples(\n",
    "        model, sde, letter_to_class[c1], class_time_to_branch_tensor, sampler=\"pc\",\n",
    "        t_limit=all_t_flip[t_i], t_start=all_t_flip[t_i + 1],\n",
    "        num_samples=orig_c2.shape[0], initial_samples=last_c2_to_c1,\n",
    "        num_steps=50\n",
    "    )\n",
    "    backward_c2_to_c1[t_i] = last_c2_to_c1.cpu().numpy()\n",
    "    \n",
    "# Compute correlation of features before and after transmutation\n",
    "c1_to_c2_corrs, c2_to_c1_corrs = np.empty(num_features), np.empty(num_features)\n",
    "for i in range(num_features):\n",
    "    c1_to_c2_corrs[i] = scipy.stats.pearsonr(\n",
    "        forward_c1_to_c2[0][:, i], backward_c1_to_c2[-1][:, i]\n",
    "    )[0]\n",
    "    c2_to_c1_corrs[i] = scipy.stats.pearsonr(\n",
    "        forward_c2_to_c1[0][:, i], backward_c2_to_c1[-1][:, i]\n",
    "    )[0]\n",
    "    \n",
    "# Plot distribution trajectory in transmutation\n",
    "fig, ax = plt.subplots(nrows=2, sharex=True, sharey=True, figsize=(20, 10))\n",
    "forward_cmap = plt.get_cmap(\"Reds_r\")\n",
    "backward_cmap = plt.get_cmap(\"Blues\")\n",
    "for t_i in range(len(all_t)):\n",
    "    ax[0].hist(\n",
    "        np.ravel(forward_c1_to_c2[t_i]), bins=60, histtype=\"step\", color=forward_cmap(t_i / len(all_t)),\n",
    "        alpha=0.5, density=True\n",
    "    )\n",
    "    ax[1].hist(\n",
    "        np.ravel(forward_c2_to_c1[t_i]), bins=60, histtype=\"step\", color=forward_cmap(t_i / len(all_t)),\n",
    "        alpha=0.5, density=True\n",
    "    )\n",
    "for t_i, t in enumerate(all_t_flip[1:]):\n",
    "    ax[0].hist(\n",
    "        np.ravel(backward_c1_to_c2[t_i]), bins=60, histtype=\"step\", color=backward_cmap(t_i / len(all_t)),\n",
    "        alpha=0.5, density=True\n",
    "    )\n",
    "    ax[1].hist(\n",
    "        np.ravel(backward_c2_to_c1[t_i]), bins=60, histtype=\"step\", color=backward_cmap(t_i / len(all_t)),\n",
    "        alpha=0.5, density=True\n",
    "    )\n",
    "ax[0].set_title(\"%s to %s\" % (c1, c2))\n",
    "ax[1].set_title(\"%s to %s\" % (c2, c1))\n",
    "ax[1].set_xlabel(\"x\")\n",
    "ax[0].set_ylabel(\"p(x)\")\n",
    "ax[1].set_ylabel(\"p(x)\")\n",
    "ax[0].set_ylim((0, 3))\n",
    "plt.show()\n",
    "\n",
    "# Plot correlation of features before and after transmutation\n",
    "num_features = input_shape[0]\n",
    "num_cols = 4\n",
    "num_rows = int(np.ceil(num_features / num_cols))\n",
    "for class_1, class_2, forwards, backwards in [\n",
    "    (c1, c2, forward_c1_to_c2[0], backward_c1_to_c2[-1]),\n",
    "    (c2, c1, forward_c2_to_c1[0], backward_c2_to_c1[-1])\n",
    "]:\n",
    "    fig, ax = plt.subplots(ncols=num_cols, nrows=num_rows, figsize=(num_cols * 5, num_rows * 5))\n",
    "    for i in range(num_features):\n",
    "        r, c = i // num_cols, i % num_cols\n",
    "        x, y = forwards[:, i], backwards[:, i]\n",
    "        corr = scipy.stats.pearsonr(x, y)[0]\n",
    "        ax[r][c].scatter(x, y, alpha=0.5)\n",
    "        ax[r][c].text(\n",
    "            0.01, 0.99, \"%.4f\" % corr,\n",
    "            ha=\"left\", va=\"top\", transform=ax[r][c].transAxes\n",
    "        )\n",
    "        ax[r][c].set_xlabel(\"Before forward diffusion\")\n",
    "        ax[r][c].set_ylabel(\"After backward diffusion\")\n",
    "        ax[r][c].set_title(\"Feature %d\" % (i + 1))\n",
    "    fig.suptitle(\"%s to %s\" % (class_1, class_2))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
